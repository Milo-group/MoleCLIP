{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saliency visualization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def plot_saliency(images, img_idx = None):\n",
    "\n",
    "    saliency, _ = torch.max(images.grad.data.abs(), dim=1)  # Take max over channels\n",
    "        \n",
    "    saliency_img = saliency.squeeze().cpu().numpy()[img_idx]\n",
    "\n",
    "    # Compute 16x16 patch-wise saliency (ViT token-wise)\n",
    "    patch_size = 16\n",
    "    num_patches = 224 // patch_size  # 14x14 for ViT-base\n",
    "\n",
    "    saliency_patches = np.zeros((num_patches, num_patches))\n",
    "\n",
    "    # Calculate mean values for 16x16 patches\n",
    "    for i in range(num_patches):\n",
    "        for j in range(num_patches):\n",
    "            patch = saliency_img[i * patch_size:(i + 1) * patch_size, j * patch_size:(j + 1) * patch_size]\n",
    "            saliency_patches[i, j] = patch.mean()\n",
    "\n",
    "    # Normalize saliency for visualization\n",
    "    saliency_patches = (saliency_patches - saliency_patches.min()) / (saliency_patches.max() - saliency_patches.min())\n",
    "\n",
    "    # Upsample back to 224x224\n",
    "    saliency_upsampled = torch.tensor(saliency_patches).unsqueeze(0).unsqueeze(0)  # Add batch & channel dims\n",
    "    saliency_upsampled = F.interpolate(saliency_upsampled, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "    saliency_img = saliency_upsampled.squeeze().numpy()\n",
    "\n",
    "    # print (saliency)\n",
    "\n",
    "    # Convert image to numpy\n",
    "    image_np = images[img_idx].permute(1,2,0).cpu().detach().numpy()  # Normalize to [0,1] for blending\n",
    "\n",
    "    # Create saliency heatmap\n",
    "    cmap = plt.get_cmap(\"hot\")\n",
    "    saliency_colored = cmap(saliency_img)[:, :, :3]  # Remove alpha channel\n",
    "\n",
    "    # Blend saliency map with image using alpha blending\n",
    "    opacity = 0.8  # Adjust opacity (0 = invisible, 1 = full heatmap)\n",
    "    overlay = (1 - opacity) * image_np + opacity * saliency_colored\n",
    "\n",
    "    # Display results\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    ax[0].imshow(image_np)\n",
    "    ax[0].axis(\"off\")\n",
    "    ax[0].set_title(\"Original Image\")\n",
    "\n",
    "    ax[1].imshow(overlay)\n",
    "    ax[1].axis(\"off\")\n",
    "    ax[1].set_title(f\"Saliency Map Overlay (Opacity={opacity})\")\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saliency for Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset, labels_len = Load_finetuning_dataset(\"../Datasets/Finetuning\", \"NHCs\", \"images\", shuffle = False)\n",
    "print (\"datasets were sucssesfully loaded\")\n",
    "\n",
    "model = Model(\"ViT-B/16\").to(\"cuda\")\n",
    "print (\"model was sucssesfully loaded\")\n",
    "\n",
    "cp_path = '##pretrained_cp##' ### add checkpoint that was trained on the desired dataset\n",
    "\n",
    "checkpoint = torch.load(cp_path, map_location=torch.device(\"cpu\"), weights_only=False)\n",
    "\n",
    "\n",
    "checkpoint['model'] = {key: value for key, value in checkpoint['model'].items() if 'cls' not in key and 'clip_model' not in key}\n",
    "\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "ff_head = create_mlp(inner_layers = 3, inner_dim = 512, dropout_rate = 0, output_dim = labels_len).to(\"cuda\")\n",
    "ff_head.load_state_dict(checkpoint['head'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "loss_func = nn.L1Loss() \n",
    "epoch_it = iter(train_dataset)\n",
    "\n",
    "images, labels = next(epoch_it)\n",
    "\n",
    "images.requires_grad = True \n",
    "\n",
    "embeddings = model.model_image(images).float()\n",
    "output = ff_head(embeddings)\n",
    "\n",
    "loss = loss_func(output, labels)\n",
    "loss.backward()\n",
    "\n",
    "\n",
    "for i in range(images.shape[0]):\n",
    "    plot_saliency(images, i)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saliency for pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data.Dataloaders import Load_contrastive_dataset\n",
    "\n",
    "train_dataset, val_dataset, classes = Load_contrastive_dataset(\"../Datasets/Pretraining\", \"chembl_25\", \"smiles\", batch_size = 32)\n",
    "print (\"datasets were sucssesfully loaded\")\n",
    "\n",
    "model = Model(\"ViT-B/16\", classes = classes).to(\"cuda\")\n",
    "print (\"model was sucssesfully loaded\")\n",
    "\n",
    "checkpoint = torch.load('../Checkpoints/MoleCLIP/MoleCLIP - Primary.pth', map_location=torch.device(\"cpu\"), weights_only=False)\n",
    "\n",
    "model.load_state_dict(checkpoint['model'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluations.pretraining_eval import features_to_logits\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "labels_template = torch.arange(32).to(\"cuda\")\n",
    "\n",
    "\n",
    "epoch_it = iter(train_dataset)\n",
    "        \n",
    "image1, image2, cls1_labels, cls2_labels = next(epoch_it)\n",
    "\n",
    "image1.requires_grad = True \n",
    "image2.requires_grad = True \n",
    "\n",
    "image1_features = model.model_image(image1).float()\n",
    "image2_features = model.model_image(image2).float()\n",
    "\n",
    "cls_1_preds = model.cls_1(image1_features)\n",
    "cls_2_preds = model.cls_2(image1_features)\n",
    "\n",
    "labels = labels_template[:image1.shape[0]]\n",
    "\n",
    "logits_per_image1, logits_per_image2 = features_to_logits (model, image1_features, image2_features, 15)\n",
    "\n",
    "loss_singles = (loss_function(logits_per_image1, labels) + loss_function(logits_per_image2, labels))/2 \n",
    "\n",
    "cls1_loss = loss_function(cls_1_preds, cls1_labels) \n",
    "cls2_loss = loss_function(cls_2_preds, cls2_labels)\n",
    "loss = loss_singles #+ cls1_loss + cls2_loss\n",
    "\n",
    "loss.backward()\n",
    "print (loss)\n",
    "\n",
    "print (image1.shape)\n",
    "\n",
    "for i in range(image1.shape[0]):\n",
    "\n",
    "    plot_saliency(image1, i)\n",
    "    plot_saliency(image2, i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MoleCLIP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
